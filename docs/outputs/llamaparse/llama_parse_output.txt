    arXiv:2305.05665v2 [cs.CV] 31 May 2023

    IMAGEBIND: One Embedding Space To Bind Them All

    Rohit Girdhar∗  Alaaeldin El-Nouby∗  Zhuang Liu  Mannat Singh
    Kalyan Vasudev Alwala  Armand Joulin             Ishan Misra∗
                    FAIR, Meta AI
                    https://facebookresearch.github.io/ImageBind

    1) Cross-Modal Retrieval
           Audio            Images & Videos    Depth                                 Text
                                                                           “A fire crackles while a pan of food is frying on
                                                                           the fire.”
    Crackle of a Fire                                                      “Fire is crackling then wind starts blowing.”
                                                                           “Firewood crackles then music...”

                                                                           “A baby is crying while a toddler is laughing.”
                                                                           “A baby is laughing while an adult is laughing.”
    Baby Cooing                                                            “A baby laughs and something…”

    2)              )
           Embedding-Space Arithmetic          3) Audio to Image Generation

    Figure 1. I  Waves    Dog    Engine    Fire    Rain    M
                 MAGEBIND’s joint embedding space enables novel multimodal capabilities. By aligning six modalities’ embedding into a
    common space, IMAGEBIND enables: 1) Cross-Modal Retrieval, which shows emergent alignment of modalities such as audio, depth or
    text, that aren’t observed together. 2) Adding embeddings from different modalities naturally composes their semantics. And 3) Audio-to-
    Image generation, by using our audio embeddings with a pre-trained DALLE-2 [61] decoder designed to work with CLIP text embeddings.

We   ) Abstract
     present IMAGEBIND, an approach to learn a joint
embedding across six different modalities - images, text, au-
dio, depth, thermal, and IMU data. We show that all combi-
nations of paired data are not necessary to train such a joint
embedding, and only image-paired data is sufficient to bind
the modalities together. IMAGEBIND can leverage recent
large scale vision-language models, and extends their zero-
shot capabilities to new modalities just by using their natu-
ral pairing with images. It enables novel emergent applica-
tions ‘out-of-the-box’ including cross-modal retrieval, com-
posing modalities with arithmetic, cross-modal detection
and generation. The emergent capabilities improve with the
strength of the image encoder and we set a new state-of-the-
art on emergent zero-shot recognition tasks across modal-
ities, outperforming specialist supervised models. Finally,
we show strong few-shot recognition results outperforming
prior work, and that IMAGEBIND serves as a new way to
                                                           )
evaluate vision models for visual and non-visual tasks.


∗Equal technical contribution.
1. Introduction
A single image can bind together many experiences – an
image of a beach can remind us of the sound of waves, the
texture of the sand, a breeze, or even inspire a poem. This
‘binding’ property of images offers many sources of super-
vision to learn visual features, by aligning them with any
of the sensory experiences associated with images. Ideally,
for a single joint embedding space, visual features should
be learned by aligning to all of these sensors. However, this
requires acquiring all types and combinations of paired data
with the same set of images, which is infeasible.
Recently, many methods learn image features aligned
with text [1, 31, 46, 60, 64, 65, 82, 83], audio [3, 4, 50,
55, 56, 70] etc. These methods use a single pair of modal-
ities or, at best, a few visual modalities. However, the fi-
nal embeddings are limited to the pairs of modalities used
for training. Thus, video-audio embeddings cannot directly


be used for image-text tasks and vice versa. A major ob-
stacle in learning a true joint embedding is the absence of
large quantities of multimodal data where all modalities are
present together.
---
                In this paper, we present IMAGEBIND, which learns a
single shared representation space by leveraging multiple
types of image-paired data. It does not need datasets where
all modalities co-occur with each other. Instead, we lever-
age the binding property of images and we show that just
aligning each modality’s embedding to image embeddings
leads to an emergent alignment across all of the modalities.
In practice, IMAGEBIND leverages web-scale (image, text)
paired data and combines it with naturally occurring paired
data such as (video, audio), (image, depth)  etc. to learn
a single joint embedding space. This allows IMAGEBIND
to implicitly align the text embeddings to other modalities
such as audio, depth etc., enabling zero-shot recognition ca-
pabilities on that modality without explicit semantic or tex-
tual pairing. Moreover, we show that it can be initialized
with large-scale vision-language models such as CLIP [60],
thereby leveraging the rich image and text representations
of these models.                Thus, IMAGEBIND can be applied to a
variety of different modalities and tasks with little training.
We use large-scale image-text paired data along with nat-
urally paired ‘self-supervised’ data across four new modal-
ities - audio, depth, thermal, and Inertial Measurement Unit
(IMU) readings – and show strong emergent zero-shot clas-
sification and retrieval performance on tasks for each of
these modalities. These emergent properties improve as the
underlying image representation is made stronger. On au-
dio classification and retrieval benchmarks, IMAGEBIND’s
emergent zero-shot classification matches or outperforms
specialist models trained with direct audio-text supervision
on benchmarks like ESC, Clotho, AudioCaps. IMAGEBIND
representations also outperform specialist supervised mod-
els on few-shot evaluation benchmarks. Finally, we show
that IMAGEBIND’s joint embeddings can be used for a wide
variety of compositional tasks as illustrated in Figure 1, in-
cluding cross-modal retrieval, combining embeddings via
arithmetic, detecting audio sources in images, and generat-
ing images given audio input.

2. Related Work
                  IMAGEBIND builds upon several advances in vision-
language, multimodal, and self-supervised research.
Language Image Pre-training.     Training images jointly
with linguistic signals like words or sentences has been
shown to be an effective method for zero-shot,                open-
vocabulary recognition and text to image retrieval [14, 18,
38, 68].            Language as supervision can further be used for
learning strong video representations [2, 47, 48]. Joulin et
al. [34] show that using large-scale image dataset with noisy
captions yields strong visual features. Recently, CLIP [60],
ALIGN [31] and Florence [83] collect large collections of
image and text pairs and train models to embed image and
language inputs in a joint space using contrastive learning,
exhibiting impressive zero-shot performance.              CoCa [82]
adds an image captioning objective on top of the contrastive
loss for improved performance. Flamingo [1] handles arbi-
trarily interleaved images and texts, and achieves state of the
art on many few-shot learning benchmarks. LiT [84] adopts
contrastive training for fine-tuning and observes freezing
image encoders works the best.     This prior line of works
mostly considers image and text, while our work enables
zero-shot recognition on multiple modalities.
Multi-Modal Learning. Our work binds multiple modal-
ity representations in a joint embedding space. Prior works
explored joint training of multiple modalities in a super-
vised [21, 42] or self-supervised contexts [3, 20, 50, 70, 74].
The success of image and language pre-training methods
such as CLIP has inspired approaches that revisits learn-
ing deep semantic representations through matching other
modalities with linguistic inputs. Various methods adapt
CLIP to extract semantically strong video representations
[15, 43, 45, 79]. Most related to our method, Nagrani et
al. [51] create a weakly-labeled dataset for paired video-
audio and captions that allows for training multi-modal
video-audio encoder to match textual features resulting in
strong audio and video retrieval and captioning perfor-
mance. AudioCLIP [27] adds audio as an additional modal-
ity into a CLIP framework, enabling zero-shot audio classi-
fication. In contrast, IMAGEBIND does not require explicit
paired data between all modalities and instead leverages im-
age as a natural weak supervision for unifying modalities.
Feature Alignment Pre-trained CLIP models have been
utilized as teachers to supervise other models due to the
strength of its visual representations [44, 58, 75]. More-
over, CLIP joint image and text embedding space has also
been leveraged for a variety of zero-shot tasks like de-
tection [24, 88], segmentation [41], mesh animation [81]
etc. showing the power of joint embedding spaces. Point-
CLIP [85] finds a pre-trained CLIP encoder can be used for
3D recognition by projecting a point cloud to a number of
2D depth map views, which in turn are encoded using CLIP
visual encoder. In multilingual neural machine translation,
a similar phenomenon to the emergence behavior of IM-
AGEBIND is commonly observed and utilized: if languages
are trained in the same latent space through learned implicit
bridging, translation can be done between language pairs on
which no paired data is provided [33, 40].
3. Method

Our goal is to learn a single joint embedding space for all
modalities by using images to bind them together. We align
each modality’s embedding to image embeddings, such as
text to image using web data and IMU to video using video
data captured from egocentric cameras with IMU. We show
that the resulting embedding space has a powerful emer-
gent zero-shot behavior that automatically associates pairs
of modalities without seeing any training data for that spe-
---
                                                     Naturally Aligned    IMAGEBIND
    Images Videos  Text  Audio Depth Thermal IMU     Emergent Alignment
    Web Image-Text       Depth Sensor Data          Web Videos            Thermal Data    Egocentric Videos

    Figure 2. IMAGEBIND overview. Different modalities occur naturally aligned in different data sources, for instance images+text and
    video+audio in web data, depth or thermal information with images, IMU data in videos captured with egocentric cameras, etc. IMAGE-
    BIND links all these modalities in a common embedding space, enabling new emergent alignments and capabilities.

    cific pair. We illustrate our approach in Figure 2.                 are optimized using an InfoNCE [54] loss:

3.1. Preliminaries


Aligning specific pairs of modalities.  Contrastive learn-
ing [28] is a general technique for learning an embedding
space by using pairs of related examples (positives) and un-
related examples (negatives).      Using pairs of aligned ob-
servations, contrastive learning can align pairs of modal-
ities such as (image, text) [60], (audio, text) [27], (image,
depth) [70], (video, audio) [50] etc. However, in each case,
the joint embeddings are trained and evaluated using the
same pairs of modalities. Thus, (video, audio) embeddings
are not directly applicable for text-based tasks while (image,
text) embeddings cannot be applied for audio tasks.
Zero-shot image classification          using text   prompts.
CLIP [60] popularized a ‘zero-shot’ classification task
based on an aligned (image, text) embedding space. This
involves constructing a list of text descriptions that describe
the classes in a dataset. An input image is classified based
on its similarity to the text descriptions in the embedding
space.      Unlocking such zero-shot classification for other
modalities requires specifically training using paired text
data, e.g., (audio, text) [27] or (point-clouds, text) [85]. In
contrast, IMAGEBIND unlocks zero-shot classification for
modalities without paired text data.

3.2. Binding modalities with images

IMAGEBIND uses pairs of modalities (I , M), where I
represents images and M is another modality, to learn a sin-
gle joint embedding. We use large-scale web datasets with
(image, text) pairings that span a wide range of semantic
concepts. Additionally, we use the natural, self-supervised
pairing of other modalities – audio, depth, thermal, and In-
tertial Measurement Unit (IMU) – with images.
     Consider the pair of modalities (I , M) with aligned ob-
servations. Given an image Ii and its corresponding obser-
vation in the other modality Mi, we encode them into nor-
malized embeddings: qi = f(Ii) and ki = g(Mi) where
f, g are deep networks. The embeddings and the encoders
                      exp(q⊺ki/τ )
                                 i
LI,M = − log exp(q⊺ki/τ ) + P              exp(q⊺kj /τ ) , (1)
where τ is a scalar   i           j=i      i
of the softmax   temperature that controls the smoothness
                 distribution and j denotes unrelated observa-
tions, also called ‘negatives’. We follow [76] and consider
every example j = i in the mini-batch to be a negative. The
loss makes the embeddings qi and ki closer in the joint em-
bedding space, and thus aligns I and M. In practice, we
use a symmetric loss LI,M + LM,I.
Emergent alignment of unseen pairs of modalities.          IM-
AGEBIND uses modalities paired with images, i.e., pairs of
the form (I , M) to align each the embeddings from each
modality M to those from images.            We observe an emer-
gent behavior in the embedding space that aligns two pairs
of modalities (M1, M2) even though we only train using
the pairs (I , M1) and (I , M2). This behavior allows us
to perform a wide variety of zero-shot and cross-modal re-
trieval tasks without training for them.   We achieve state-
of-the-art zero-shot text-audio classification results without
observing a single sample of paired (audio, text).
3.3. Implementation Details
             IMAGEBIND is conceptually simple and can be imple-
mented in many different ways. We deliberately choose a
vanilla implementation that is flexible and allows for an ef-
          X
fective study and easy adoption. In § 5, we present design
decisions that are critical for good emergent ‘binding’.
Encoding modalities.       We use a Transformer architec-
ture [73] for all the modality encoders. We use the Vision
Transformer (ViT) [13] for images. Following [20], we use
the same encoder for images and videos.           We temporally
inflate [7] the patch projection layer of the ViT and use 2
frame video clips sampled from 2 seconds. We follow [22]
for encoding audio and convert a 2 second audio sampled at
16kHz into spectrograms using 128 mel-spectrogram bins.
As the spectrogram is also a 2D signal like an image, we use
a ViT with a patch size of 16 and stride 10. We treat ther-
mal images and depth images as one-channel images and
---
Dataset                               Task  #cls Metric         #test
Audioset Audio-only (AS-A) [19]    Audio cls.    527    mAP     19048
ESC 5-folds (ESC) [59]             Audio cls.     50    Acc      400
Clotho (Clotho) [17]               Retrieval      -    Recall    1045
AudioCaps (AudioCaps) [37]         Retrieval      -    Recall    796
VGGSound (VGGS) [8]                Audio cls.    309    Acc     14073
SUN Depth-only (SUN-D) [69]        Scene cls.     19    Acc      4660
NYU-v2 Depth-only (NYU-D) [66]     Scene cls.     10    Acc      653
LLVIP (LLVIP) [32]                Person cls.     2     Acc     15809
Ego4D (Ego4D) [23]                 Scenario cls. 108    Acc     68865
pretrained vision (ViT-H 630M params) and text encoders
(302M params) from OpenCLIP [11, 30].
Encoders for each modality.      We convert audio into 2D

mel-spectrograms [22], and thermal and depth modalities
into 1 channel images and use ViT-B, ViT-S encoders re-
spectively. The image and text encoders are kept frozen
during the IMAGEBIND training and the audio, depth, ther-
mal, and IMU encoders are updated.


    Table 1. Emergent zero-shot classification datasets for audio,                 Emergent zero-shot vs. zero-shot.       Methods such as
    depth, thermal, and Inertial Measurement Unit (IMU) modalities.                CLIP [60], AudioCLIP [27] etc. train with modality pairs,
    We evaluate IMAGEBIND without training for any of these tasks                  (image, text) and (audio, text), to demonstrate zero-shot
    and without training on paired text data for these modalities. For             classification using text-prompts for the same modality. In
    each dataset, we report the task (classification or retrieval), number         contrast, IMAGEBIND binds modalities together using only
    of classes (#cls), metric for evaluation (Accuracy or mean Average             image-paired data. Thus, just by training on (image, text)
    Precision), and the number of test samples (#test).                            and (image, audio), IMAGEBIND    can perform zero-shot
                                                                                   classification of audio using text prompts. As we do not
                                                                                   directly train for this ability, we term it emergent zero-shot
    also use a ViT to encode them. We follow [21] to convert                       classification to distinguish it from methods that specifically
    depth into disparity maps for scale invariance. We extract                     train using paired text-supervision for all modalities.
    the IMU signal consisting of accelerometer and gyroscope                       Evaluation on downstream tasks.    We comprehensively
    measurements across the X , Y , and Z axes. We use 5 sec-                      evaluate IMAGEBIND on a many different downstream
    ond clips resulting in 2K time step IMU readings which are                     tasks using different protocols. We summarize the main
    projected using a 1D convolution with a kernel size of 8.                      datasets used for evaluation in Table 1.
    The resulting sequence is encoded using a Transformer. Fi-
    nally, we follow the text encoder design from CLIP [60].                       4.1. Emergent zero-shot classification
                       We use separate encoders for images, text, audio, ther-     We evaluate IMAGEBIND on emergent zero-shot classi-
    mal images, depth images, and IMU. We add a modality-                          fication and use the text prompt templates from [60] (full
    specific linear projection head on each encoder to obtain a                    details in Appendix B). We report the results in Table 2.
    fixed size d dimensional embedding, that is normalized and                     Each task measures IMAGEBIND’s ability to associate text
    used in the InfoNCE loss from Eq 1. In addition to ease of                     embeddings to the other modalities without observing them
    learning, this setup allows us to also initialize a subset of                  together during training. Given the novelty of our problem
    the encoders using pretrained models, e.g., the image and                      setting, there are no “fair” baselines to compare IMAGE-
    text encoder using CLIP [60] or OpenCLIP [30].                                 BIND with.  Nevertheless, we compare to prior work that

    4. Experiments                                                                 uses text paired with certain modalities (e.g. audio [27, 51]),
                                                                                   and for certain “visual-like” modalities such as depth and
                        We first describe the main experimental setup and pro-     thermal, we use the CLIP model directly.    We also report
    vide full details in the supplement.                                           the best reported supervised upper bound per benchmark.
    Naturally paired modalities and datasets.              We use IM-              IMAGEBIND achieves a high emergent zero-shot clas-
                         AGEBIND on six modalities - image/video, text, audio,     sification performance. On each benchmark, IMAGEBIND
    depth, thermal images, and IMU. As described in § 3.3, we                      achieves strong gains and even compares favorably to super-
    treat videos as 2 frame images and process them the same                       vised specialist models trained for the specific modality and
    as images. For the naturally available paired data, we use                     task. These results demonstrate that IMAGEBIND aligns the
    the (video, audio) pairs from the Audioset dataset [19], (im-                  modalities and implicitly transfers the text supervision as-
    age, depth) pairs from the SUN RGB-D dataset [69], (im-                        sociated with images to other modalities like audio. In par-
    age, thermal) pairs from the LLVIP dataset [32] and (video,                    ticular, IMAGEBIND shows strong alignment for non-visual
    IMU) pairs from the Ego4D dataset [23]. For these pairs of                     modalities like audio and IMU suggesting that their natu-
    modalities, we do not use any extra supervision like class la-                 rally available pairing with images is a powerful source of
    bels, text etc. Since SUN RGB-D and LLVIP are relatively                       supervision. For completeness, we also report the standard
    small, we follow [21] and replicate them 50× for training.                     zero-shot image (ImageNet [63] - IN1K, Places-365 [87] -
    Large scale image-text pairs. We leverage image-text su-                       P365) and video (Kinetics400 [35] - K400, MSR-VTT 1k-
    pervision from large-scale web data [60]. For ease of ex-                      A [78] - MSR-VTT) tasks. As the image & text encoders
    perimentation, we use pretrained models that are trained                       are initialized (and frozen) using OpenCLIP, these results
    on billions of (image, text) pairs. Specifically, we use the                   match those of OpenCLIP.
---
                      IN1K      P365               K400         MSR-VTT             NYU-D     SUN-D         AS-A      VGGS       ESC     LLVIP     Ego4D
     Random           0.1          0.27            0.25             0.1              10.0      5.26         0.62    0.32         2.75     50.0      0.9
     IMAGEBIND        77.7         45.4            50.0          36.1                54.0      35.1         17.6    27.8         66.9     63.4      25.0
     Text Paired       -         -                  -                -              41.9∗     25.4∗  28.4† [27]      -         68.6† [27]  -         -
     Absolute SOTA 91.0 [82] 60.7 [67]          89.9 [80]        57.7 [79]        76.7 [21] 64.9 [21]    49.6 [39]  52.5 [36]  97.0 [9]    -         -

    Table 2. Emergent zero-shot classification of IMAGEBIND using text prompts highlighted in blue. IMAGEBIND aligns images with text,
    depth, audio, thermal and IMU modalities. The resulting embedding space can associate text embeddings with the non-image modalities,
    and leads to strong emergent zero-shot classification. We show strong performance even on non-visual modalities such as audio and IMU.
    We compare to ‘Text Paired’ baselines wherever possible, which trains with paired text data for that modality. ∗We use the OpenCLIP ViT-
    H [30] on depth rendered as grayscale images. †[27] that uses AS class names as supervision during training, and hence is not “zero-shot”.
    Overall, IMAGEBIND shows strong emergent zero-shot performance, even compared to such upper bounds. We also report the absolute
    state-of-the-art (SOTA) on each dataset for reference, which typically uses additional supervision, model ensembles etc. We report the
    top-1 classification accuracy for all datasets except MSR-VTT (Recall@1) and Audioset Audio-only (mAP).

                           Emergent    Clotho            AudioCaps        ESC                               Modality Emergent              MSR-VTT

  Uses audio and text supervision  R@1 R@10 R@1 R@10 Top-1
  AudioCLIP [27]          ✗                                           68.6
  Uses audio and text loss
  AVFIC [51]              ✗     3.0         17.5     8.7     37.7
  No audio and text supervision
  IMAGEBIND               ✓     6.0         28.4     9.3     42.3     66.9
  Supervised
  AVFIC finetuned [51]    ✗     8.4         38.6
  ARNLQ [53]              ✗        12.6     45.4    24.3     72.1

Table 3. Emergent zero-shot audio retrieval and classification.
We compare IMAGEBIND to prior work on zero-shot audio re-
trieval and audio classification. Without using audio-specific su-
pervision, IMAGEBIND outperforms prior methods on zero-shot
retrieval and has comparable performance on the classification
task. IMAGEBIND’s emergent zero-shot performance approaches
those of specialist supervised models.


4.2. Comparison to prior work
 MIL-NCE [49]             V      ✗        R@1 R@5 R@10
 SupportSet [57]          V      ✗        8.6      16.9     25.8
 FIT [5]                  V      ✗        10.4     22.2     30.0
 AVFIC [51]          A+V         ✗        15.4     33.6     44.1
 I       B                A      ✓        19.4     39.5     50.3
    MAGE      IND                         6.8      18.5     27.2
 IMAGEBIND           A+V         ✗        36.8     61.8     70.0
Table 4. Zero-shot text based retrieval   on MSR-VTT 1K-A.

We compare IMAGEBIND’s emergent retrieval performance using
audio and observe that it performs favorably to methods that use
the stronger video modality for retrieval.


pervised’.    IMAGEBIND’s strong performance on all three
benchmarks validates its ability to align the audio and text
modalities using images as a bridge.
Text to audio and video retrieval. We use the MSR-VTT
1k-A benchmark to evaluate the text to audio and video re-
trieval performance in Table 4. Only using audio, IMAGE-

      We now compare IMAGEBIND against prior work in                                   BIND achieves strong emergent retrieval performance com-
    zero-shot retrieval and classification tasks.                                      pared to the video retrieval performance of prior work like
                                                                                       MIL-NCE. The text to video performance for our model is
    Zero-shot text to audio retrieval and classification.                 Un-          strong (36.1% R@1 in Table 2) as it uses OpenCLIP’s vi-
    like IMAGEBIND, prior work trains using paired data for                            sion and text encoders and outperforms many prior meth-
    that modality,                     e.g., AudioCLIP [27] uses (audio, text) su-     ods.           However, combining the audio and video modalities
    pervision and AVFIC [52] uses automatically mined (au-                             further boosts performance showing the utility of IMAGE-
    dio, text) pairs. We compare their zero-shot text to audio                         BIND’s features over an already strong retrieval model.
    retrieval and classification performance to IMAGEBIND’s                            4.3. Few-shot classification
    emergent retrieval and classification in Table 3.
                             IMAGEBIND significantly outperforms prior work on the      We now evaluate the label-efficiency of IMAGEBIND by
    audio text retrieval benchmarks. On the Clotho dataset, IM-                        evaluating on few-shot classification. We use the audio and
    AGEBIND has double the performance of AVFIC despite not                            depth encoders from IMAGEBIND and evaluate them on au-
    using any text pairing for audio during training. Compared                         dio and depth classification respectively in Figure 3.       For
    to the supervised AudioCLIP model, IMAGEBIND achieves                              ≥1-shot results, we follow [50, 60] and train linear classi-
    comparable audio classification performance on ESC. Note                           fiers on fixed features (details in Appendix B).
    that AudioCLIP uses class names from AudioSet as text                               On few-shot audio classification (Figure 3 left), we com-
    targets for audio-text training, hence is referred to as ‘su-                      pare with (1) self-supervised AudioMAE model trained
---
                                               40
       80
Top-1                                          30                                               Chirping birds
       60                                     Top-1
(Fold-1)
                                              SUN-D
       40                                      20
ESC             IMAGEBIND                                                                       Claps
       20      AudioMAE [77]                   10           IMAGEBIND
               Supervised [77]                              MultiMAE [4]
             0  1  2  4    8                       0  1  2  4                8
                      # shots per class                     # shots per class                   Church Bells

Figure 3. Few-shot classification on audio and depth. We report
the emergent zero-shot classification performance on each bench-                                Thunderstorm
mark (denoted by ⋆). We train linear classifiers on fixed features
for the ≥ 1-shot case.                  (Left) In all settings, IMAGEBIND outper-     Figure 4. Embedding space arithmetic where we add image
forms the self-supervised AudioMAE model. IMAGEBIND even                              and audio embeddings, and use them for image retrieval. The
outperforms a supervised AudioMAE model upto 4 shot learning                          composed embeddings naturally capture semantics from different
showing its strong generalization.                    (Right) We compare with the     modalities. Embeddings from an image of fruits + the sound of
MultiMAE model trained with images, depth, and semantic seg-                          birds retrieves images of birds surrounded by fruits.
mentation masks. IMAGEBIND outperforms MultiMAE across all
few-shot settings on few-shot depth classification.

on audio from Audioset and (2) a supervised AudioMAE
model finetuned on audio classification.                    Both baselines
use the same capacity ViT-B audio encoder as IMAGE-
BIND.              IMAGEBIND                    significantly outperforms the Au-     Dog barking             Sea waves  Keyboard typing   Clock alarm
dioMAE model on all settings with gains of ∼40% accuracy
in top-1 accuracy on ≤4-shot classification.                            IMAGEBIND     Figure 5. Object detection with audio queries. Simply replacing
also matches or outperforms the supervised model on ≥1-                               Detic [88]’s CLIP-based ‘class’ embeddings with our audio em-
shot classification. IMAGEBIND’s emergent zero-shot per-                              beddings leads to an object detector promptable with audio. This
formance surpasses the supervised ≤2-shot performance.                                requires no re-training of any model.
For few-shot depth classification, we compare with the
multimodal MultiMAE [4] ViT-B/16 model trained on im-                                 pretrained text-based detection model, Detic [88], and sim-
ages, depth, and semantic segmentation data. IMAGEBIND                                ply replace its CLIP-based ‘class’ (text) embeddings with
significantly outperforms MultiMAE across all the few-shot                            IMAGEBIND’s audio embeddings.        Without training, this
settings.                        Altogether, these results show the strong gener-     creates an ‘audio’-based detector that can detect and seg-
alization of IMAGEBIND audio and depth features trained                               ment objects based on audio prompts. As shown in Fig-
with image alignment.                                                                 ure 5, we can prompt the detector with the barking sound of

4.4.            55    12   A
             Analysis and Applications
                11          2                                                         a dog to localize a dog.
         10                                                                           Upgrading text-based diffusion models to audio-based.
Multimodal embedding space arithmetic.                                   We study     We use a pretrained DALLE-2 [61] diffusion model (private
whether I                  B               3
   9                MAGE                     IND’s embeddings can be used to com-     reimplementation) and replace its prompt embeddings by
pose information across modalities. In Figure 4, we show                              our audio embeddings. In Figure 1, we observe that we can
             8                             4₂₀
image retrievals obtained by adding together image and au-                            repurpose the diffusion model to generate plausible images
                      Patille  5
dio embeddings. The joint embedding space allows for us to
                    7 6 125                                                           using different types of sounds.
compose two embeddings: e.g., image of fruits on a table +
sound of chirping birds and retrieve an image that contains                           5. Ablation Study                                       photo by yizhen
both these concepts,                       i.e., fruits on trees with birds. Such
emergent compositionality whereby semantic content from                               We investigate various design choices for learning a joint
different modalities can be composed will likely enable a
CHENERY                                                                               embedding space for different modalities. Since the abla-
800
rich variety of compositional tasks.                                                  tion experimental setup is similar to § 4, we only note the
                      050
                            Without re-training, we can ‘upgrade’ existing vision
models              LIPPARD                                                           main differences (full details in Appendix C). We report re-
                   that use CLIP embeddings to use IMAGEBIND em-                      sults on the ESC fold-1 for the ablation study. We use a ViT-
beddings LIPPARD
                    from other modalities such as audio.                              B encoder for the image, audio, depth, and thermal modali-
                    51& 46 T0 01
Upgrading text-based detectors to audio-based. We use a                               ties by default and train them for 16 epochs (vs. 32 epochs
                    #

                    50 & 56 T0 84
                    #
---
                                                                                   temperature τ ( Eq 1) in Table 5a. We experiment with a
     50                                 64                                         learnable temperature initialized to 0.07 (parametrized in
     48                             Fold-1                                         the log-scale) following [60] vs. various values of fixed tem-
    NYU-D                                                                          peratures. Unlike [60], we observe that a fixed temperature
     46                                 62                                         is best for depth, audio and IMU classification. Addition-
     44ViT-B ViT-L  ViT-H          ESC ViT-B ViT-L           ViT-H                 ally, we see that a higher temperature is better for train-

                                                                                   ing the depth, thermal, and IMU encoders, whereas a lower
     62                             24                                             temperature works best for the audio modality.
                                    22                                             Projection head. We vary the projection head used for each
    LLVIP                          Ego4D
     60                             20                                             encoder from a linear layer to an MLP with 768 hidden di-

 58                             18
 ViT-B ViT-L    ViT-H           ViT-B ViT-L             ViT-H

Figure 6. Scaling the image encoder size while keeping the other
modality encoders’ size fixed.               We measure the performance on
the emergent zero-shot classification of depth, audio, thermal, and
IMU modalities. Scaling the image encoder significantly improves
the zero-shot classification results suggesting that a stronger visual
representation improves the ‘binding’ of modalities.
mensions. The results in Table 5b show that a linear pro-
jection performs better for both modalities. This is in con-
trast to standard self-supervised methods like SimCLR [10]
whose performance improves with MLP projection heads.
Training epochs. We vary the number training epochs and
report the classification performance in Table 5c. Longer
training consistently improves the emergent zero-shot per-
formance for both modalities across all datasets.
Data augmentation for paired images.          During IM-


in § 4). For IMU we use a lightweight 6 layer encoder
with 512 dimensional width and 8 heads, and train it for 8
epochs. The text encoder follows [60] and is a twelve layer
Transformer with a width of 512 dimensions. We initialize
the image and text encoder from the CLIP model [60].
5.1. Scaling the Image Encoder

                      The central idea in IMAGEBIND is aligning the embed-
dings of all modalities to image embeddings. Thus, the im-
age embeddings plays a central role in the emergent align-
ment of unseen modalities and we study their effect on the
emergent zero-shot performance.                    We vary the size of the
image encoder and train an encoder for the depth, audio
etc. modalities to match the image representation. To iso-
late the effect of the image representation, we fix the size
of the other modality encoders. We use the pretrained CLIP
(ViT-B and ViT-L) and OpenCLIP (ViT-H) image and text
encoders for this experiment. Our results in Figure 6 show
that IMAGEBIND’s emergent zero-shot performance on all
modalities improves with better visual features. For depth
and audio classification, the stronger ViT-H vs. the ViT-B
image encoder, provides a gain of 7% and 4% respectively.
Thus, stronger visual features can improve recognition per-
formance even on non-visual modalities.
5.2. Training Loss and Architecture

                 We study the effect of the training design choices on the
emergent zero-shot classification. We focus on two modali-
ties with different characteristics - depth which is visual and
spatial, and audio which is non-visual and has a temporal
component. We found that studying these diverse modali-
ties led to robust and transferable design decisions.
Contrastive loss temperature.                   We study the effect of the
          AGEBIND training, we augment images either using ba-
sic augmentation (cropping, color jitter) or strong aug-
mentation that additionally applies RandAugment [12] and
RandErase [86]. We specify the augmentation parameters
in Appendix C. Stronger augmentation helps depth classifi-
cation when training on the small number of (image, depth)
pairs from the SUN RGB-D dataset. However, for audio,
strongly augmenting the video makes the task too challeng-
ing, leading to a significant drop of 34% on ESC.
Depth specific design choices. We vary the type of spatial
crops used for training in Table 5e. Following CMC [70],
we use two unaligned random crops from the correspond-
ing image and depth pair       vs. our default choice of using
spatially aligned random crops. Contrary to CMC, we ob-
serve that random cropping severely degrades performance:
more than 10% on SUN-D. Unlike vanilla self-supervised
learning, our image representations learned from image-
text pairs are more semantic and thus spatially misaligned
crops hurt performance. In Table 5f, we observe that Ran-
domErase [86] boosts performance on depth classification.
Audio specific design choices.        We train for video-audio
alignment using temporally aligned samples or unaligned
samples and measure the final performance in Table 5g.
Similar to the depth classification observation, temporally
aligned samples lead to better performance. Table 5h shows
that using frequency masking augmentation for audio also
provides a small boost in performance.
Capacity of the audio and depth encoders and their im-
pact of the classification performance is reported in Table 6.
A smaller encoder for depth improves performance pre-
sumably because of the relatively small size of the (image,
depth) dataset. Conversely, we observe that larger audio en-
coder improves the performance, particularly when paired
with a high capacity image encoder.
---
    Temp → Learn 0.05 0.07 0.2         1.0            Proj head → Linear MLP                Epochs → 16  32         64                Data aug →  Basic Strong
    SUN-D    24.1  27.0 27.3 26.7 28.0                SUN-D       26.7        26.5          SUN-D  26.7 27.9 29.9                       SUN-D     25.4    26.7
     ESC     54.8  56.7 52.4 45.4 24.3                  ESC       56.7        51.0          ESC    56.7 61.3 62.9                        ESC      56.7    22.6
    (a) Temperature for loss.                         (b) Projection Head.                  (c) Training epochs.                     (d) Data aug for image.
             Spatial align → None Aligned          Data aug →  None RandErase          Temporal align→   None Aligned                Data aug → Basic +Freq mask
                  SUN-D  16.0  26.7                  SUN-D     24.2       26.7              ESC          55.7       56.7              ESC     56.5        56.7
    (e) Spatial alignment of depth.                   (f) Depth data aug.              (g) Temporal alignment of audio.               (h) Audio data aug.

    Table 5. Training loss and architecture design decisions and their impact on emergent zero-shot classification. Settings for results in § 4
    highlighted in gray. (a) A fixed temperature in the contrastive loss outperforms a learnable one for all modalities. (b) A linear projection
    head for computing the depth or audio embedding works better than an MLP head. (c) Longer training improves the zero-shot classification
    performance for both modalities.          (d) Stronger image augmentation improves depth classification while basic augmentation significantly
    improves audio classification.     (e, f) Using spatially aligned image and depth crops when training IMAGEBIND significantly improves
    performance. Similarly, RandErase augmentation is critical to good zero-shot classification on depth. (g, h) Temporally aligned audio and
    video matching gives improved performance and using frequency augmentation for audio gives a slight improvement.

                   Audio Encoder (ESC) Depth Encoder (SUN)                                           IN1K      VGGS ESC              SUN-D NYU-D
    Image Encoder  ViT-S                      ViT-B    ViT-S      ViT-B                     DINO [6]   64.4         17.2     44.7     26.8     48.8
        ViT-B       52.8                       56.7     30.7       26.7                     DeiT [72] 74.4†         9.6      25.0     25.2     48.0
    ViT-H           54.8                       60.3     33.3       29.5
                                                                                       Table 8. IMAGEBIND as an evaluation tool. We initialize (and
    Table 6.                    Capacity of the audio and depth encoders and their     fix) the image encoder with different methods and align other
    impact on performance. A stronger image encoder improves per-                      modalities. IMAGEBIND measures the impact of visual features
    formance for both audio and depth tasks. As the number of (image,                  on multimodal tasks. † trained with IN1K supervision.
    depth) pairs is small, a smaller encoder improves performance for
    depth. For audio classification, a larger encoder is better.

             Batch size →    512              1k       2k      4k                      6. Discussion and Limitations
                NYU-D        47.3             46.5    43.0    39.9                     IMAGEBIND is a simple and practical way to train a joint
                 ESC         39.4             53.9    56.7    53.9                     embedding space using only image alignment. Our method

Table 7. Effect of scaling batch size. We found the optimal batch
size for contrastive loss varied by the modality. For image-depth
task, a smaller batch size was better, likely due to the small size
and limited diversity of the original dataset. For audio-video task
where we have a lot more positive and negative audio-video pairs,
using a large batch size lead to better results.

Effect of batch size.                     In Table 7 we evaluate the effect of
batch size on the representation learned.           As shown, the
batch size can vary across modalities depending on the size
and complexity of the corresponding pretraining datasets.
                  IMAGEBIND to evaluate pretrained vision models in Ta-
ble 8.                     We initialize the vision encoder using a pretrained
model and keep it fixed. We use image-paired data to align
and train text, audio, and depth encoders (full details in Ap-
pendix B). Compared to the supervised DeiT model, the
self-supervised DINO model is better at emergent zero-shot
classification on both depth and audio modalities.               More-
over, the emergent zero-shot performance is not correlated
with the pure vision performance on ImageNet suggest-
ing that these tasks measure different properties. IMAGE-
               BIND can serve as a valuable tool to measure vision models’
strength on multimodal applications.
leads to emergent alignment across all modalities which
can be measured using cross-modal retrieval and text-based
zero-shot tasks. We enable a rich set of compositional mul-
timodal tasks across different modalities, show a way to
evaluate pretrained vision models for non-vision tasks and
‘upgrade’ models like Detic and DALLE-2 to use using au-
dio. There are multiple ways to further improve IMAGE-
BIND. Our image alignment loss can be enriched by using
other alignment data, for instance other modalities paired
with text, or with each other (e.g. audio with IMU). Our
embeddings are trained without a specific downstream task,
and thus lag the performance of specialist models. More re-
search into adapting general purpose embeddings for each
task, including structured prediction tasks such as detection
will be beneficial. Finally, new benchmarks, e.g. our emer-
gent zero-shot task to measure emergent abilities of multi-
modal models, would help create exciting new applications.
Our model is a research prototype and cannot be readily
used for real world applications ( Appendix F).
Acknowledgements:       Authors would like to thank Uriel
Singer, Adam Polyak and Naman Goyal for their help with
the DALLE-2 experiments, and the entire Meta AI team for
many helpful discussions.
---
References
 [1] Jean-Baptiste Alayrac, Jeff Donahue,   Pauline Luc,     An-
  toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
  Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
  Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
  Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-

  bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
  hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
  Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
  Flamingo: a visual language model for few-shot learning.
  In NeurIPS, 2022. 1, 2
 [2] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider,
  Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lu-
  cas Smaira, Sander Dieleman, and Andrew Zisserman. Self-
  supervised multimodal versatile networks. NeurIPS, 2020.
  2
 [3] Relja Arandjelovic and Andrew Zisserman. Look, listen and
  learn. In ICCV, 2017. 1, 2
 [4] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir
  Zamir. MultiMAE: Multi-modal Multi-task Masked Autoen-
 arXiv preprint arXiv:2106.11097, 2021. 2
[16] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim-
 ing He. Masked autoencoders as spatiotemporal learners. In
 NeurIPS, 2022. 13
[17] Frederic Font, Gerard Roma, and Xavier Serra. Freesound
 technical demo. In ACM MM, 2013. 4, 12
[18] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
 Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De-
 vise: A deep visual-semantic embedding model.      NeurIPS,
 2013. 2
[19] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
 Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
 and Marvin Ritter.  Audio set:       An ontology and human-
 labeled dataset for audio events. In ICASSP, 2017. 4, 12
[20] Rohit    Girdhar,  Alaaeldin  El-Nouby,  Mannat     Singh,
 Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.
 OmniMAE: Single Model Masked Pretraining on Images
 and Videos. In CVPR, 2023. 2, 3
[21] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der
 Maaten, Armand Joulin, and Ishan Misra. Omnivore: A Sin-
 gle Model for Many Visual Modalities. In CVPR, 2022. 2,

     [5] coders. In ECCV, 2022. 1, 6                                      4, 5, 12
      Max Bain, Arsha Nagrani, G¨
                                        ul Varol, and Andrew Zisser-     [22] Yuan Gong, Yu-An Chung, and James Glass.     AST: Audio
      man. Frozen in time: A joint video and image encoder for            Spectrogram Transformer. In Interspeech, 2021. 3, 4, 13
      end-to-end retrieval. In ICCV, 2021. 5                             [23] Kristen  Grauman,  Andrew     Westbury,  Eugene     Byrne,
     [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´
                                                         e J´
                                                               egou,      Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
      Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-          Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-
      ing properties in self-supervised vision transformers.      In      tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar

     [7] ICCV, 2021. 8                                                    Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,
      Jo˜
           ao Carreira and Andrew Zisserman.    Quo vadis, action         Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant
      recognition? A new model and the kinetics dataset. In CVPR,         Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien

     [8] 2017. 3                                                          Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichten-
      Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zis-            hofer, Adriano Fragomeni, Qichen Fu, Abrham Gebrese-
      serman.       Vggsound: A large-scale audio-visual dataset. In      lasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei

     [9] ICASSP, 2020. 4, 12                                              Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kot-
      Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-             tur, Anurag Kumar, Federico Landini, Chao Li, Yanghao
      Kirkpatrick, and Shlomo Dubnov.      Hts-at:    A hierarchical      Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu,
      token-semantic audio transformer for sound classification           Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will
      and detection. In ICASSP, 2022. 5                                   Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari,
          [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-      Kiran Somasundaram, Audrey Southerland, Yusuke Sugano,
      offrey Hinton. A simple framework for contrastive learning          Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma
      of visual representations. In ICML, 2020. 7, 14                     Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Cran-
         [11] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell      dall, Dima Damen, Giovanni Maria Farinella, Christian Fue-
      Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-            gen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar,
      mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-          Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe,
      ing laws for contrastive language-image learning. In CVPR,          Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato,
      2023. 4                                                             Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo
         [12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V      Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around
      Le.         Randaugment: Practical automated data augmentation      the world in 3,000 hours of egocentric video. In CVPR, 2022.
      with a reduced search space. In CVPR, 2020. 7                       4, 12
         [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,     [24] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
      Dirk   Weissenborn,       Xiaohua Zhai,   Thomas   Unterthiner,     Open-vocabulary object detection via vision and language
      Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-            knowledge distillation. In ICLR, 2022. 2
      vain Gelly, et al.  An image is worth 16x16 words: Trans-          [25] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik.    Per-
      formers for image recognition at scale. In ICLR, 2021. 3            ceptual organization and recognition of indoor scenes from
    [14] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja       rgb-d images. In CVPR, 2013. 13
      Fidler.       VSE++:      Improving Visual-Semantic Embeddings     [26] Saurabh Gupta, Ross Girshick, Pablo Arbel´
      with Hard Negatives. In BMVC, 2018. 2                               Malik. Learning rich features from        aez, and Jitendra
    [15] Han Fang,  Pengfei     Xiong,  Luhui   Xu, and  Yu Chen.         detection and segmentation. In         rgb-d images for object
      Clip2video:   Mastering video-text retrieval via image clip.                                          ECCV, 2014. 13
---
[27] Andrey Guzhov, Federico Raue, J¨
                                             orn Hees, and Andreas
 Dengel.              AudioCLIP: Extending CLIP to Image, Text and
 Audio. arXiv preprint arXiv:2106.13043, 2021. 2, 3, 4, 5
[28] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-
 ality reduction by learning an invariant mapping. In CVPR,
 2006. 3
[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
 Weinberger. Deep networks with stochastic depth. In ECCV,
 2016. 14
[30] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
 Gordon,     Nicholas  Carlini,   Rohan  Taori,  Achal       Dave,
 Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
 naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
 clip, 2021. 4, 5
[31] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
 Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
 Duerig. Scaling up visual and vision-language representation
 learning with noisy text supervision. In ICML, 2021. 1, 2
[32] Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli
 Zhou. Llvip: A visible-infrared paired dataset for low-light
 vision. In ICCV, 2021. 4, 12, 13
[33] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun,
 Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´
                                                       egas,
 Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jef-
 frey Dean. Google’s multilingual neural machine translation
 system: Enabling zero-shot translation. In ACL, 2017. 2
[34] Armand Joulin, Laurens van der Maaten, Allan Jabri, and
 Nicolas Vasilache.            Learning visual features from large
 weakly supervised data. In ECCV, 2016. 2
[35] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
 Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
 Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,
 and Andrew Zisserman.             The kinetics human action video
 dataset. arXiv preprint arXiv:1705.06950, 2017. 4
[36] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and
 Dima Damen. Slow-fast auditory streams for audio recogni-
 tion. In ICASSP, 2021. 5
[37] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
 Gunhee Kim. Audiocaps: Generating captions for audios in
 the wild. In NAACL, 2019. 4, 12
[38] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel.
 Unifying visual-semantic embeddings with multimodal neu-
 ral language models. In NeurIPS Workshop, 2014. 2

[39] Khaled Koutini, Jan Schl¨
                       uter, Hamid Eghbal-zadeh, and Ger-
 hard Widmer. Efficient training of audio transformers with
 patchout. In Interspeech, 2022. 5
[40] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and
 Marc’Aurelio Ranzato.            Unsupervised machine translation
 using monolingual corpora only. In ICLR, 2018. 2
[41] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
 Koltun, and Ren´
                     e Ranftl.       Language-driven semantic seg-
 mentation. In ICLR, 2022. 2
[42] Valerii Likhosherstov, Anurag Arnab, Krzysztof Choroman-
 ski, Mario Lucic, Yi Tay, Adrian Weller, and Mostafa De-
 hghani. Polyvit: Co-training vision transformers on images,
 videos and audio. arXiv preprint arXiv:2111.12993, 2021. 2
[43] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de
 Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng
 Li. Frozen clip models are efficient video learners. In ECCV,
 2022. 2

[44] Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and
 Rongrong Ji.      Exploring target representations for masked
 autoencoders. arXiv preprint arXiv:2209.03917, 2022. 2
[45] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
 Nan Duan, and Tianrui Li. CLIP4Clip: An Empirical Study
 of CLIP for End to End Video Clip Retrieval. arXiv preprint
 arXiv:2104.08860, 2021. 2
[46] Dhruv Mahajan, Ross    Girshick, Vignesh      Ramanathan,
 Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
 and Laurens Van Der Maaten. Exploring the limits of weakly
 supervised pretraining. In ECCV, 2018. 1
[47] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan
 Laptev, Josef Sivic, and Andrew Zisserman.   End-to-end
 learning of visual representations from uncurated instruc-
 tional videos. In CVPR, 2020. 2
[48] Antoine Miech, Dimitri Zhukov,   Jean-Baptiste Alayrac,
 Makarand     Tapaswi,   Ivan  Laptev,  and   Josef   Sivic.
 Howto100m: Learning a text-video embedding by watching
 hundred million narrated video clips. ICCV, 2019. 2
[49] Antoine Miech, Dimitri Zhukov,   Jean-Baptiste Alayrac,
 Makarand     Tapaswi,   Ivan  Laptev,  and   Josef   Sivic.
 Howto100m: Learning a text-video embedding by watching

 hundred million narrated video clips. In ICCV, 2019. 5
[50] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-
 visual instance discrimination with cross-modal agreement.
 In CVPR, 2021. 1, 2, 3, 5
[51] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
 Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
 Learning audio-video modalities from image captions.     In
 ECCV, 2022. 2, 4, 5
[52] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,
 Cordelia Schmid, and Chen Sun. Attention bottlenecks for
 multimodal fusion. In NeurIPS, 2021. 5
[53] Andreea-Maria Oncescu, A Koepke,        Joao F Henriques,
 Zeynep Akata, and Samuel Albanie.      Audio retrieval with
 natural language queries. In Interspeech, 2021. 5, 12
[54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.  Rep-
 resentation learning with contrastive predictive coding. In
 NeurIPS, 2018. 3
[55] Andrew Owens and Alexei A Efros.   Audio-visual scene
 analysis with self-supervised multisensory features.     In
 ECCV, 2018. 1
[56] Mandela Patrick, Yuki M Asano, Ruth Fong, Jo˜
                                              ao F Hen-

 riques, Geoffrey Zweig, and Andrea Vedaldi.  Multi-modal
 self-supervision from generalized data transformations.  In
 ICCV, 2021. 1
[57] Mandela  Patrick, Po-Yao  Huang, Yuki   Asano,  Florian
 Metze, Alexander Hauptmann, Joao Henriques, and Andrea
 Vedaldi.     Support-set bottlenecks for video-text representa-
 tion learning. In ICLR, 2021. 5

[58] Zhiliang Peng,    Li Dong, Hangbo  Bao,  Qixiang     Ye,
 and Furu     Wei.        BEiT v2:  Masked   image modeling
 with vector-quantized visual tokenizers.       arXiv preprint
 arXiv:2208.06366, 2022. 2
[59] Karol J Piczak. Esc: Dataset for environmental sound clas-
 sification. In ACM MM, 2015. 4, 12
[60] Alec Radford,  Jong Wook Kim,    Chris Hallacy,  Aditya
 Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
---
 Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
 ing transferable visual models from natural language super-
 vision. In ICML, 2021. 1, 2, 3, 4, 5, 7, 13, 14, 15
[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
 and Mark Chen.            Hierarchical text-conditional image gen-
 eration with clip latents.        arXiv preprint arXiv:2204.06125,
 2022. 1, 6, 13
[62] Ren´
 e       Ranftl,     Katrin Lasinger,  David Hafner,      Konrad
 Schindler, and Vladlen Koltun.            Towards robust monocular
 depth estimation: Mixing datasets for zero-shot cross-dataset
 transfer.       TPAMI, 2020. 13
[63] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
 jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
 Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
 Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
 lenge. IJCV, 2015. 4
[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
 Cade    Gordon,     Ross   Wightman,   Mehdi Cherti,          Theo
 Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
 man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
 Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
 Jitsev. LAION-5B: An open large-scale dataset for training
 next generation image-text models. In NeurIPS Datasets and
 Benchmarks, 2022. 1
[65] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
 Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
 Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
 Open dataset of clip-filtered 400 million image-text pairs. In
 NeurIPS Workshop, 2021. 1
[66] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
 Fergus.             Indoor segmentation and support inference from
 rgbd images. In ECCV, 2012. 4, 12, 13
[67] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius
 de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv
 Mahajan, Ross Girshick, Piotr Doll´
 Maaten. Revisiting weakly                  ar, and Laurens van der
 perception models. In            supervised pre-training of visual
[68] Richard                CVPR, 2022. 5
                  Socher, Andrej Karpathy, Quoc V Le, Christopher D
 Manning, and Andrew Y Ng.               Grounded compositional se-
 mantics for finding and describing images with sentences.
 ACL, 2014. 2
[69] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
 Sun rgb-d: A rgb-d scene understanding benchmark suite. In
 CVPR, 2015. 4, 12
[70] Yonglong Tian, Dilip Krishnan, and Phillip Isola.         Con-
 trastive multiview coding. arXiv preprint arXiv:1906.05849,
 2019. 1, 2, 3, 7
[71] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
 Videomae: Masked autoencoders are data-efficient learners
 for self-supervised video pre-training. In NeurIPS, 2022. 13
[72] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
 Massa, Alexandre Sablayrolles, and Herv´
                                             e J´
 data-efficient image transformers &          egou. Training
                                           distillation through at-

 tention. In ICML, 2021. 8
[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
 reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
 Polosukhin. Attention is all you need. In NeurIPS, 2017. 3
[74] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen,
 Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou,
 and Lu Yuan. BEVT: Bert pretraining of video transformers.
 In CVPR, 2022. 2
[75] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao,
 Jianmin Bao, Dong Chen, and Baining Guo.          Contrastive
 learning rivals masked image modeling in fine-tuning via
 feature distillation. arXiv preprint arXiv:2205.14141, 2022.
 2

[76] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
 Unsupervised feature learning via non-parametric instance
 discrimination. In CVPR, 2018. 3
[77] Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Woj-
 ciech Galuba, Florian Metze, and Christoph Feichtenhofer.
 Masked autoencoders that listen. In NeurIPS, 2022. 6
[78] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
 video description dataset for bridging video and language. In
 CVPR, 2016. 4
[79] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua
 Song, Houqiang Li, and Jiebo Luo. CLIP-ViP: Adapting Pre-
 trained Image-Text Model to Video-Language Representa-
 tion Alignment. In ICLR, 2023. 2, 5
[80] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi
 Zhang, Chen Sun, and Cordelia Schmid. Multiview trans-
 formers for video recognition. In CVPR, 2022. 5
[81] Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh. Clip-actor:
 Text-driven recommendation and stylization for animating
 human meshes. In ECCV, 2022. 2
[82] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
 jtaba Seyedhosseini, and Yonghui Wu.        Coca: Contrastive
 captioners are image-text foundation models. TMLR, 2022.
 1, 2, 5
[83] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
 Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
 Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
 Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
 Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
 Pengchuan Zhang.      Florence: A new foundation model for
 computer vision. arXiv preprint arXiv:2111.11432, 2021. 1,
 2
[84] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
 Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
 Lit: Zero-shot transfer with locked-image text tuning.  In
 CVPR, 2022. 2
[85] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
 peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
 Li. Pointclip: Point cloud understanding by clip. In CVPR,

 2022. 2, 3
[86] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
 Yi Yang. Random erasing data augmentation. In AAAI, 2020.
 7
[87] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
 ralba, and Aude Oliva.       Learning deep features for scene
 recognition using places database. In NeurIPS, 2014. 4

[88] Xingyi Zhou, Rohit Girdhar, Armand Joulin,        Philipp
 Kr¨
   ahenb¨
 classes    uhl, and Ishan Misra.    Detecting twenty-thousand
            using image-level supervision. In ECCV, 2022. 2, 6,
 13
---
A. Datasets and Metrics
Audioset (AS) [19]. This dataset is used for both training
and evaluation. It contains 10s videos from YouTube anno-
tated into 527 classes. It consists of 3 pre-defined splits, the
balanced split with about 20K videos, test split with 18K
videos, and an unbalanced training split with about 2M vi-
does. For training, we use the 2M unbalanced set without
any labels, and only use it for audio-video matching. For
zero-shot evaluation in Table 2, we use the test set and
compute logits for each class using the textual class names
along with the templates as described later in Appendix B.3.
The metric used is top-1 accuracy.
ESC-50 (ESC) [59]. We use this dataset for evaluating the
learned representations in a zero-shot manner. The task here
is “Environmental Sound Classification” (ESC). It consists
of 2000 5s audio clips classified into 50 classes. It has pre-
defined 5 fold evaluation, each consisting of 400 test audio
clips. In this work, we compute 0-shot predictions on the
evaluation set for each fold and report the 5-fold average
performance.      For ablations we use only the first fold for
computational ease. The metric used is top-1 accuracy.
Clotho (Clotho) [17]. This is a dataset of audio from the
Freesound platform with textual descriptions. It consists of
a dev and test set of 2893 and 1045 audio clips respectively,
with each clip associated with 5 descriptions. We consider
the text→audio retrieval task, and consider each of the 5 as-
sociated captions as a separate test query and retrieve from
the set of audio clips. The metric used is recall@K , where
a given test query is assumed to be correctly solved if the
ground truth audio is retrieved within the top-K retrieved
audio clips.
AudioCaps (AudioCaps) [37]. This is a dataset of audio-
visual clips from YouTube accompanied by textual descrip-
tions. It consists of clips from the Audioset dataset as de-
scribed earlier. We use the splits as provided in [53],1 which
removes clips that overlap with the VGGSound dataset. We
end up with 48198 training, 418 validation and 796 test
clips. We only use the test set for zero-shot evaluation of
our model. The task is text→audio retrieval, and evaluation
is performed using recall@K .
VGGSound (VGGS) [8]. This dataset contains about 200K
video clips of 10s length, annotated with 309 sound classes
consisting of human actions, sound-emitting objects and
human-object interactions. We only use the audio from the
test set (with 14073 clips) for 0-shot classification.    The
evaluation is done using the top-1 accuracy metric.
SUN RGB-D (SUN). We use the registered RGB and Depth
maps provided in the SUN RGB-D [69] dataset train set
(∼5K pairs) for training our model. We follow [21] to post
process the depth maps in two steps - 1) we use in-filled
1https : / / www . robots . ox . ac . uk / ˜vgg / research / audio -
retrieval/resources/benchmark- files/AudioCaps_retrieval_
dataset.tar.gz
depth values and 2) convert them to disparity for scale nor-
malization. This dataset is only used in training, so we do
not use any metadata or class labels.
SUN Depth-only (SUN-D). We use only the ∼5K depth
maps from the val split of the SUN RGB-D [69] dataset
and denote them as SUN Depth-only. This dataset is only
used for evaluation and we do not use the RGB images. We
process the depth maps similar to SUN RGB-D (in-filled
depth, converted to disparity). We use the 19 scene classes
in the dataset and use their class names for constructing the
zero-shot classification templates.
NYU-v2 Depth-only (NYU-D). We use the 794 val set
depth maps from the NYU-v2 Depth-only [66] dataset
for evaluation only. We post-process the depth similar to
SUN Depth-only. We use the 10 scene class names in the
dataset. The 10th scene class, called ‘other’, correspond to
18 different semantic classes – [’basement’,   ’cafe’,
’computer lab’,      ’conference   room’, ’dinette’,
’exercise room’, ’foyer’, ’furniture store’,
’home    storage’, ’indoor balcony’, ’laundry
room’, ’office kitchen’, ’playroom’, ’printer
room’, ’reception room’, ’student lounge’,
’study’, ’study room’].  For         zero-shot evaluation,
we compute the cosine similarity of the 10th class as the
maximum cosine similarity among these 18 classnames.
LLVIP (LLVIP). The LLVIP dataset [32] consists of RGB
image and Thermal (infrared low-light) image pairs. The
dataset was collected in an outdoor setting using fixed cam-
eras observing street scenes and contains RGB images taken
in a low-light paired with infrared images (8∼14um fre-
quency).          The RGB thermal pairs are registered in the
dataset release.  For training, we use the train set with
12025 RGB image and thermal pairs.   For evaluation,
we use the   val  set with 3463 pairs of RGB and ther-
mal images.  Since the original dataset is designed for
detection, we post process it for a binary classification
task.          We crop out pedestrian bounding boxes and ran-
dom bounding boxes (same aspect ratio and size as pedes-
trian) to create a balanced set of 15809 total boxes (7931
‘person’ boxes).     For zero-shot classification, we use the
following class names for the ‘person’ class -   [’per-
son’,    ’man’, ’woman’, ’people’], and            [’street’,
’road’, ’car’, ’light’, ’tree’] for the background
class.
Ego4D (Ego4D) [23]. For the Ego4D dataset, we consider
the task of scenario classification. There are 108 unique sce-
narios present in the 9,645 videos of the Ego4D dataset. We
filter out all videos annotated with more than one scenario
which yields 7,485 videos with a single scenario assigned.
For each video, We select all time-stamps that contains a
synchronized IMU signal as well as aligned narrations. We
sample 5 second clips around each time-stamp. The dataset
is split randomly such that we have 510,142 clips for train-
---
ing, and 68,865 clips for testing. During training we only
use the video frames and their corresponding IMU signal.
We use the test split to measure zero-shot scenario classi-
fication performance, where each clip of IMU signal is as-
signed the video-level scenario label as its ground-truth.
A.1. Data Representations

                  We use the standard RGB and RGBT representations
for images and videos. For videos, we use 2-frame clips,
inspired from recent work on ViT-style video architec-
tures [16, 71], where a video patch is 2×16×16 (T×H×W ).
We inflate the visual encoder’s weights to work with spa-
tiotemporal patches and and at inference time we aggregate
features over multiple 2-frame clips.            Hence, we can use
models trained on image-text data directly on videos.
               We used a single-channel image for the thermal data
since it is the natural form in which current infrared thermal
sensors return data [32].            For single-view depth, we ex-
perimented with different encodings – absolute depth [66]
as returned by sensors like the Kinect, inverse depth [62],
disparity [62], and HHA [25, 26]. Overall, we found that
disparity representation (which is a single-channel image)
worked the best. For audio we use the raw waveform pro-
cessed into mel-spectrograms [22], as described in the main
text. For IMU we use a 6 × T           tensor to represent the se-
quence of IMU sensor readings over time.
where k ∈ {1, 2, 4, 8}.  We fix the k samples such that
our model and the baselines use exactly the same samples
during training. For all few-shot evaluations, including the
baselines, we freeze the encoder parameters and only train
a linear classifier.
Audio:  For audio few-shot training with ESC, our model
and the baselines are trained using AdamW with a learning
rate of 1.6 × 10−3 and weight decay of 0.05 for 50 epochs.
Depth:  For depth few-shot training with SUN, our model
and the baselines are trained using AdamW with a learning
rate of 10−2 and no weight decay for 60 epochs.
B.3. Zero-shot evaluation details

Query Templates.    For all evaluations, we use the default
set of templates from CLIP [60].2 Note that we use the same
templates for non visual modalities like audio and depth as
well since we only use semantic/textual supervision associ-
ated with images.
B.4. Qualitative evaluation details

Cross-modal nearest neighbors.                     We perform the re-
trieval on the embedding feature after temperature scaling.
The nearest neighbors are computed using cosine distance.
In Figure 1, we show retrievals for audio from ESC, image
retrievals from IN1K and COCO, depth from SUN-D, and
text from AudioCaps.

B. Evaluation details

          We now describe the evaluation setups used in this work.
B.1. Inference implementation details

Audio/Video: For both these temporal modalities (whether
operated upon together during pre-training or separately
during inference), we sample fixed length clips to operate
on. During training, we randomly sample a clip, typically
2s in length. At inference time, we uniformly sample multi-
ple clips to cover the full length of the input sample. For in-
stance, for 5s ESC videos, we would sample ⌈ 5 ⌉ = 3 clips.
For video clips, we sample a fixed number   2
each clip. For audio, we                            of frames from
by sampling it at                  process each raw audio waveform
                            16KHz followed by extracting a log mel
spectrogram with 128 frequency bins using a 25ms Ham-
ming window with hop length of 10ms. Hence, for a t sec-
ond audio we get a 128 ×100t dimensional input.
IMU: For IMU, we sample fixed length clips of 5 seconds,
centered around time-stamps that are aligned with narra-
tions. For each clip, we get a 6×2000 dimensional input and
we measure the zero-shot performance for scenario classifi-
cation using each clip as an independent testing sample.
B.2. Few-shot evaluation details
For the few-shot results in Figures 3 using the ESC and
SUN datasets, we sampled k   training samples per class,
Embedding arithmetic.                For arithmetic, we again use the
embedding features after temperature scaling. We ℓ2 nor-
malize the features and sum the embeddings after scaling
them by 0.5. We use the combined feature to perform near-
est neighbor retrieval using cosine distance, as described
above. In Figure 1, we show combination of images and
audio from IN1K and ESC, and show retrievals from IN1K.
Audio→Image Generation.  For generating images form
audio clips, we rely on an in-house reproduced implemen-
tation of DALLE-2 [61]. In DALLE-2, to produce images
from text prompts, the image generation model relies on
text embeddings produced by the pre-trained CLIP-L/14
text encoder. Since IMAGEBIND naturally aligns CLIP’s-
embedding space to that of other modalities proposed in the
paper, we can upgrade the DALLE-2 model to generate im-
ages by prompting it with these new unseen modalities. We
achieve zero-shot audio to image generation with DALLE-2
by simply using the temperature-scaled audio embeddings
generated by IMAGEBIND’s audio encoder as a proxy for
the CLIP’s text embeddings in the DALLE-2’s image gen-
eration model.
Detecting objects using audio. We extract all audio de-
scriptors from the validation set of ESC using an IMAGE-
            BIND ViT-B/32 encoder, yielding 400 descriptors in total.
We use an off-the-shelf CLIP-based Detic [88] model and
2https : / / github . com / openai / CLIP / blob / main / notebooks /
Prompt_Engineering_for_ImageNet.ipynb
---
use the audio descriptors as the classifier for Detic in place               Text query: ”Cooking a meal”
of CLIP text-based ‘class’ embeddings.                We use a score
threshold of 0.9 for the qualitative results in Figure 5.

C. Pretraining details
C.1. Best setup
               In Table 9 we detail the hyperparameters used to pre-
train each of the models reported in Table 4. Our experi-                    Text query: ”A person doing gardening work outdoors”
ments were done on 32GB V100 or 40GB A100 GPUs.

 Config                  AS        SUN        LLVIP         Ego4D
 Vision encoder              ViT-Huge
 embedding dim.         768        384         768     512
 number of heads         12         8           12      8
 number of layers        12         12          12      6
 Optimizer                    AdamW
 Optimizer Momentum    β₁ = 0.9, β₂ = 0.95
 Peak learning rate    1.6e-3     1.6e-3       5e-4    5e-4
 Weight decay           0.2        0.2         0.05    0.5                  Figure 7.   IMU retrievals.  Given a text query, we show some
 Batch size             2048       512         512     512
 Gradient clipping      1.0        1.0         5.0     1.0                  IMU retrievals and corresponding video frames.
 Warmup epochs                              2
 Sample replication     1.25        50          25     1.0
 Total epochs            64         64          64      8                   C.2. Ablation setup
 Stoch. Depth [29]      0.1        0.0         0.0     0.7
 Temperature            0.05       0.2         0.1     0.2                       The following setup was used for our evaluations in § 5.
 Augmentations:
       RandomResizedCrop                    224px                           Different from the best setup, all ablation experiments uses
         size                    Bilinear    Bilinear                       ViT-Base both for the vision and the modality-specific en-
         interpolation           p = 0.5     p = 0.5                        coders. The models are trained for 16 epochs, unless men-
       RandomHorizontalFlip      p = 0.25    p = 0.25                       tioned otherwise.
       RandomErase
       RandAugment                9/0.5       9/0.5                              For Table 5b, the differences between the linear and MLP
       Color     Jitter        12  0.4         0.4                          heads are detailed below: The MLP head did not improve
       Frequency   masking                                                  performance in our experiments.

                  Table 9. Pretraining hyperparameters                       Linear     Linear(in dim,  out dim)
                                                                              MLP       Linear(in dim,  in dim), GELU, Linear(in dim, out dim)

                      H
                                                   Acc.
                  1.0                              Acc.
Contrastive 1.0
trastive          loss batch size    vs. modalities.        While con-      D. Additional Results
               losses do require larger batch size, this requirement
                  0.5
                  0.5
didn’t increase with the number of modalities.              As noted
                  0.0
in                 0.0                                                      Qualitative results. We show additional results (along with
 Appendix B, our experiments (Table 2) sample a mini-                       audio) in the accompanying video.
                  -0.5
batch of         -0.5
                  one pair of modalities at a time: batch size of 2K        Practical applications of disparate modalities.            In gen-
for (video,       -1.0
                 audio), and 512 for (image, depth), (image, ther-
                 -1.0      0  250           750                            1750
                                            750    1000
                                                   1000     1250
                               250   500                    1250           1750  2000
                                     500                          1500
                           0                                      1500           2000
mal), and (video, IMU). These batch sizes are smaller than                  eral, a shared embedding space enables a variety of differ-
                                                   Gyro.
the               1.0                              Gyro.                    ent cross-modal search and retrieval applications. e.g., since
       >32K batch sizes used in prior work [10, 60].
                  0.75                                                      IMU sensors are ubiquitous (in phones, AR/VR headsets,
                  0.5
                  0.50                                                      health trackers), IMAGEBIND can allow a user to search
                  0.25
Combining 0.0
                 modalities.         In Table 4, we show results with       an IMU database using text queries (without training with
                  0.00
combining the audio and video modalities.             We combine            IMU-text pairs). IMU-based text search has applications
                 -0.5
                 -0.25
them by extracting embeddings from both modalities per                      in healthcare/activity search. For instance, in Figure 7 we
                 -1.0
                 -0.50
sample                     0    250  500    750
           and computing a linear combinations of those em-                 show examples of IMU (and accompanying video) retrieval
                                250  500    750    1000
                                                   1000     1250
                                                            1250  1500
                           0                                      1500     1750  2000
                                                                           1750  2000
beddings. We used a weight of 0.95 for video and 0.05 for                   given textual search query.              The retrieved IMU sample,
audio for this combination, which was found to perform the                  shown as 3-channel Accelerometer (Acc) and Gyroscope
best.                                                                       (Gyro) recording, matches the text query.
---
E. Additional Ablations
Design choices in losses. Since the modality-specific en-
coders are trained to align with a frozen image encoder, we
tried using a ℓ2 regression objective. For ZS SUN top-1
accuracy, we observed that regression led to good perfor-
mance as the sole objective (25.17%) or jointly with con-
trastive (29.04%). However, it did not improve over using
only the contrastive objective (31.74%).

F. Ethical considerations
IMAGEBIND        learns a joint embedding for   multiple
modalities. Such an embedding is intended to associate se-
mantically related concepts from different modalities. How-
ever, such an embedding may also create unintentional as-
sociations. Thus, joint embedding models, including IM-
AGEBIND must be studied carefully with a lens towards
measuring such associations, and their implications. IM-
AGEBIND leverages the image-text embeddings learned by
a pretrained model on large web-based data which has bi-
ases as documented in different studies [60]. For learning
joint embeddings for other modalities such as audio, ther-
mal, depth, and IMU we leverage datasets mentioned in Ap-
pendix A. These joint embeddings are thus limited to the
concepts present in the datasets. For example, the thermal
datasets we used are limited to outdoor street scenes, while
the depth datasets are limited to indoor scenes.